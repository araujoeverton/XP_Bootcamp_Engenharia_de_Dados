<img align="right" src="https://raw.githubusercontent.com/araujoeverton/XP_Bootcamp_Engenharia_de_Dados/main/assets/bootcamp-engenheiro-de-dados-xp.jpg" width="1080"/> ...

## Introdução
A área de tecnologia tem se incorporado ao negócio, o papel dos dados tem se tornado um ativo para as empresas e se tornado um fator determinante para seu sucesso. Com isso a demanda por engenheiros de dados tem aumentado, e a implementação d euma cultura data Driven se tornou cada vez mais frequente.
Neste módulo Fundamentos em Engenharia de Dados, foi abordado de forma mais conceitual, o papel do engenheiro de dados.

## O Profissional de Engenharia de Dados
Quando falamos em engenharia de dados é comum nos depararmos com uma série de ferramentas, que utilizamos para criar uma pipeline de dados segura, íntegra e com baixos custos.

### Hard Skills
- Conhecimento sólido em banco de dados
- Linguagem de programação como Python e Pyspark
- Conceito e fundamentos de computação em nuvem
- Propor ideias, pensar no negócio
- Otimização de performance, através de ferramentas e conhecimento

### Soft Skills
- Boa capacidade de comunicação, é essencial, transpor de forma clara as ideias e soluções
- Transmição de conhecimento
- Saber ouvir novas ideias
- Flexibilidade e adaptabilidade em caso de mudanças
- Saber lidar com pessoas
- Melhoria continua, vida ativa de estudos

### O processo da engenharia
A demanda provém de uma ideia de negócio, onde buscamos soluções através da expertise e ferramentas disponíveis na nuvem, para isso algumas eapas são importantes como:
- Documentação, com uma governança de dados implementada desde o início, pensando em manutenção, fácil entendimento do processo e aplicabilidade para toda a equipe.
- Priorização, tempo de entrega, logs para debugar processos que estejam com problema, são coisas importantes
- Arquitetura, pensando em  trazer técnicas e normas que preencham requisitos de segurança, disponibilidade dos serviços, escalabilidade, entre outros

### Pipelines de dados
Podemos definir o pipeline de dados comp um fluxo de extração dos dados, podendo ou não ser transformados para uma entrega mais efetiva, e por fim sendo disponibilizados em um local específico para consumo dos mesmos.
Podem ser classificados em modelo de <b>Batch</b> ( onde podem ser extraidos e disponibilizados em período de tempo maior ) ou em <b>Streaming</b> ( onde o fluxo é contínuo e rápido ).

### Workloads Streaming e Batch, e processos de ETL e ELT
<b>Streaming:</b>Por conta da necessidade de escalabilidade de processamento de dados, muitas empresas por exemplo, deixaram o modelo monolitico para implementar modelos de microserviços em suas aplicações. Desta forma os dados vem de diversas fontes, e tudo isso precisa ser integrado e ter uma alta disponibilidade dependendo da regra de negócio. O Streaming veio para soclunionar estes problemas. Poderiamos citar empresas como Netflix, Uber, Ifood que adotam este modelo de workload.

<b>Batch:</b> Os blocos de dados são armazenados e processados de tempos em tempos, não tendo uma necessidade tão alta de carga rápida dos dados.

<b>Micro-Batch:</b> É um meio termo entre os outros dois modelos de workloads, onde os dados são recebidos em stream, porém a carga tem um maior fluxo, não quase em real time como o Streaming, mas considerável.

*O ETL ( Extract, Transform , Load ) 
*O ELT ( Extract, Load, Transform )

### Aula prática
Neste momento reservamos um momento para o desenvolvimento de uma pipeline de dados, através do Google Colab.










  

